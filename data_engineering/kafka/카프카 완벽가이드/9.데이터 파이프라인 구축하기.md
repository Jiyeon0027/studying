# Chapter 9. 데이터 파이프라인 구축하기

- 두가지의 예시

  1. 카프카가 하나의 엔드포인트
  2. 두개의 서로다른 시스템을 연결하는 파이프라인 사이(어플리케이션 -> 카프카 -> 엘라스틱 서치)

- 카프카가 데이터 파이프라인에서 매우 크고 안정적인 버퍼의 역할을 해줌
- 읽는 쪽과 쓰는 쪽을 분리함으로써 하나의 원본에서 가져온 동일한 데이터를 서로다른 timeline, 가용성 요구조건을 가진 시스템에 전달

- 데이터 통합에서의 문제들을 해결하는 방법

## 9.1 데이터 파이프라인 구축시 고려사항

1. 적시성

   - 실시간이나 일단위 배치 작업에 이르는 모든 작업에 사용가능
   - 쓰는 쪽과 읽는 쪽 사이의 시간적 민감도에 대한 요구조건을 분리시키는 `거대한 버퍼`로 생각
   - 백프래셔의 적용이 쉬워짐

2. 신뢰성

   - 전달보장 (최소 한번 혹은 정확히 한번)
   - 자체는 최소한번 , 트랜잭션이나 고ㅇ키를 지원하는 외부 데이터 저장소와 결합되는 정확히 한번도 보장 가능

3. 높으면서도 조정 가능한 처리율

   - 확장 가능하도록 `scale-out` 되는 시스템
   - 처리율을 조정하기 쉬움 (ex. 프로듀서의 처리율이 컨슈머의 처리율을 넘어설 경우 카프카에 누적되고 마는 시스템)
   - 분산 시스템

4. 데이터 형식

   - 카프카자체와 커넥트 api는 데이터 형식에 완전히 독립적
   - 프로듀서와 컨슈머는 어떤 시리얼라이저도 사용가능
   - 카프카 커넥트는 자료형과 인메모리 객체들을 가지고 있고, 컨버터 또한 지원

5. 변환

   - `ETL` : 추출 - 변환 - 적재
     - 연산과 저장의 부담을 데이터 파이프라인으로 옮기는 단점
     - 레코드를 변환과정에서 삭제하면 가져온 데이터에서는 이전 필드를 볼 수 없어 전체 파이프라인을 수정해야함
   - `ELT` : 추출 - 적재 - 변환
     - 대상 시스템에 전달되는 데이터가 원본데이터와 최대한 비슷하도록 최소한의 변환만을 진행
     - 하나의 시스템에 데이터가 모이게 되지만 대상시스템의 cpu, 자원들을 사용

6. 보안

   - 누가 데이터에 접근하는지, 변경하는지, 문제없이 인증ㅇ을 통과할 수 있는지, 암호화가 되어있는지 ...
   - 자격증명 credential
   - 볼트와 같은 외부 비밀관리 시스템
   - 카프카 커넥트의 외부 비밀설정 지원

7. 장애처리

   - 장애시 복구가 되는지, 결함있는 레코드를 재처리 가능한지

8. 결합Coupling과 민첩성Agility
   - 데이터 원본과 대상을 분리해야함
   - **의도치 않은 커플링**
     - 임기응변 ad-hoc 파이프라인
     - 특정한 엔드포인트에 강하게 결합되어 모니터링이 어려움
   - **메타데이터의 유실**
     - 스키마 메타데이터를 보존하지 않았다면 소스와 싱크쪽 소프트웨어를 강하게 결합함
     - 모든 데이터의 파싱 및 해석에 대해 알고 있어야함
   - **과도한 처리**
     - 너무 많은 데이터를 중간에 처리하면 하단의 시스템의 선택지가 줄어들게됨

## 9.2 카프카 커넥트 vs 프로듀서/컨슈머

- 카프카 커넥트 : 카프가를 직접 코드나 api를 작성하지 않았고 변경도 할 수 없는 데이터 저장소에 연결시켜야할
  때 사용 (설정파일만 이용) - 권장
- 카프카 클라이언트 : 애플리케이션 코드를 변경할 수 있으면서 카프카에 데이터를 쓰거나 읽고 싶어할때 사용

## 9.3 카프카 커넥트

- 카프카와 다른 데이터 저장소 사이에 확장성과 신뢰성을 가지면서 데이터를 주고받을 수 있는 수단을 제공
- 카프카 플러드인을 개발하고 실행을 위한 api 와 런타임 제공
- 워커 프로세스들의 클러스터 형태로 실행
- 형식을 변경 가능하도록 convertor 사용

### 카프카 커넥트 예시

- 카프카 커넥트를 프로덕션 환경에서 사용할 경우, 카프카 브로커와는 별도의 서버에서 커넥트를 실행
- `bin/connect-distributed.sh config/connect-distibuted.properties` 실행

**설정**

- **bootstrap.servers**
  - 카프카 커넥트와 함께 장동하는 카프카 브로커의 목록
  - 커넥터는 다른 곳의 데이터를 이 브로커로 전달 혹은 브로커에서 다른 시스템으로 전송
- **group.id**
  - 동일한 그룹 ID를 갖는 모든 워커들은 같은 커넥트 클러스터를 구성
- **plugin.path**
  - 카프카 커넥트는 커넥터, 컨버터, transformation, 비밀 제공자를 다운로드 받아서 플랫폼에 플러그인할 수 있음
  - 카프카 커넥트에는 커넥터와 그 의존성들을 찾을 수 있는 디렉토리를 1개 이상 설정 가능
  - 카프카 커넥트의 클래스패스에 커넥터를 의존성과 함께 추가 가능하지만 권장하지 않음
- **key.converter와 value.converter**
  - 카프카에 저장될 메시지의 키와 밸류 부분에 각각에 대해 컨버터를 설정해 줄 수 있음
  - 컨버터마다 설정할 수 있는 매개변수 값이 따로 있음
- **rest.host.name과 rest.port**

  - 커넥터를 설정하거나 모니터링할 때는 카프카 커넥트의 REST API를 사용하는 것이 보통, REST API에 사용할 특정한 포트값을 할당 가능

    ```bash
    $ curl http://localhost:8083/
    {"version":"3.0.0-SNAPSHOT", "commit":"faed88303akdf", "kafka_cluster_id":"pfkdIELWNDm8Rtl-vALDKdg"}
    ```

**파일소스와 파일 싱크**

- 파일커넥터와 JSON 컨버터를 실행해보자

  1. 카프카 커넥트 워커 실행

  ```bash
  $ bin/connect-distributed.sh config/connect-distributed.properties & #분산모드
  ```

  2. 파일소스를 시작함

  ```bash
  $ echo '{"name":"load-kafka-config", "config":{"connector.class":"FileSreamSource","file":"config/server.properties","topic":"kafka-config-topic"}}' | curl -X POST -d @- http://localhost:8083/connectors \
      -H "Content-Type: application/json"
  {
  "name": "load-kafka-config",
  "config": {
      "connector.class": "FileStreamSource",
      "file": "config/server.properties",
      "topic": "kafka-config-topic",
      "name": "load-kafka-config"
  },
  "tasks": [
      {
      "connector": "load-kafka-config",
      "task": 0
      }
  ],
  "type": "source"
  }
  ```

  3. 카프카 설정파일이 토픽에 저장되었는지 확인

     - `config/server.properties` 파일의 내용물이 커넥터에 의해 줄 단위로json으로 변환된 뒤에 카프카 토릭에 저장되는 것임

  4. 싱크 커넥터로 토픽의 내용물을 파일로보냄

     - JSON convertor로 레코드를 텍스트 문자열로 원상복구 시키기 때문에 자동 변환

     ```bash
     $ echo '{"name":"dump-kafka-config",
                "config":{"connector.class":"FileStreamSink",
                          "file":"copy-of-server-properties",
                          "topics":"kafka-config-topic"}}' \
     | curl -X POST -d @- http://localhost:8083/connectors \
       --header "Content-Type:application/json"

     {"name":"dump-kafka-config",
      "config":{"connector.class":"FileStreamSink",
                "file":"copy-of-server-properties",
                "topics":"kafka-config-topic",
                "name":"dump-kafka-config"},
      "tasks":[]}
     ```

  5. 커넥터 삭제
     ```bash
     $ curl -X DELETE http://localhost:8083/connectors/dump-kafka-config
     ```

**CDC 변경데이터 캡쳐와 디비지움 프로젝트**

- JDBC커넥터는 JDBC와 SQL을 이용해서 데이터 베이스의 테이블에 새로들어온 레코드를 찾아냄
- 하지만 비효율적이고 정확하지 않으므로 <mark>트랜잭션 로그</mark>를 읽어 변경을 탐지하는 방법을 사용
- Debezium 프로젝트를 사용하여 커넥트

### 개별 메시지 전환

- 예시에서 MySql -> ElasticSearch로의 데이터 복사를 수행
- 변환단계에서 개별메시지를 변환하고자할 때 사용 (Single message Transformation, SMT)

- SMT 종류들

  - **Cast** : 필드의 데이터 타입을 바꿈
  - **MaskField** : 특정 필드의 내용물을 null로 채움, 민감한 정보나 개인 식별 정보를 제거할 때 유용
  - **Filter** : 특정한 조건에 부합하는 모든 메시지를 제외하거나 포함
  - **Flatten** : 중첩된 자료 구조를 편다. 각 밸류값의 경로 안에 있는 모든 필드의 이름을 이어붙인 것이 새 키 값이 됨
  - **HeaderFrom** : 메시지에 포함되어 있는 필드를 헤더로 이동시키거나 복사
  - **InsertHeader** : 각 메시지의 헤더에 정적인 문자열을 추가
  - **InsertField** : 메시지에 새로운 필드를 추가해 넣는다. 오프셋과 같은 메타데이터에서 가져온 값일 수도 있고 정적인 값일 수도 있음
  - **RegexRouter** : 정규식과 교체할 문자열을 사용해서 목적지 토픽의 이름을 바꿈
  - **ReplaceField** : 메시지에 포함된 필드를 삭제하거나 이름을 변경
  - **TimestampConverter** : 필드의 시간 형식을 바꿈
  - **TimestampRouter** : 메시지에 포함된 타임스탬프 값을 기준으로 토픽 변경. 이것은 싱크 커넥터에서 특히나 유용, 타임스탬프 기준으로 저장된 특정 테이블의 파티션에 메시지를 복사해야 할 경우, 토픽 이름만으로 목적지 시스템의 데이터세트를 찾아야 하기 때문

- **참고 자료**
  - Twelve Days of SMT : 다양한 변환에 대한 상세한 예제
  - 변환 기능 직접 개발하고 싶을 때

### 좀 더 자세히 알아보기

1. 커넥터와 태스크

   - 커넥터 플러그인은 커넥터 api를 구현함
   - 커넥터와 태스크는 <mark>데이터 이동</mark> 단계
   - 커넥터
     - 커넥터에서 몇 개의 태스크가 실행되어야 하는지 결정
     - 데이터 복사 작업을 각 태스크에 어떻게 분할해 줄지 결정
     - 워커로부터 태스크 설정을 얻어와서 태스크에 전달
   - 태스크
     - 태스크는 데이터를 실제로 카프카에 넣거나 가져오는 작업을 담당
     - 모든 태스크는 워커로부터 컨텍스트를 받아서 초기화
     - 각 컨텍스트는 각 커넥터가 초기화될 때 필요한 정보를 갖고 있음

2. 워커

   - 카프카 커넥트의 워커 프로세스는 태스크를 실행시키는 컨테이너
   - 커넥트 서러정을 내부 카프카 토픽에 저장하고, 실행시키고, 설정값 전달
   - 워커는 <mark>REST API, 설정관리, 신뢰성, 고가용성, 규모확장성, 부하 분산</mark>
   - 관심사의 분리라는 장점

3. 컨버터 및 커넥트 데이터 모델

   - 카프카 커넥터 API에는 데이터 API가 포함되어 있음. 이 API는 데이터 객체와 이 객체의 구조를 나타내는 스키마 모두를 다룸.
   - 소스 커넥터(ConnectSchema 객체를 생성)
     -> 커넥트 워커(컨버터를 이용해 카프카에 저장)
     -> 싱크 커넥터(레코드를 카프카에 저장된 형식으로 API레코드 변환)

4. 오프셋 관리
   - 워커 프로세스가 커넥터에 제공하는 편리한 기능으로 어떤 데이터를 이미 처리했는지에 대한 정보 관리
   - 소스커넥터 : 커넥터 워커에 리턴하는 레코드에는 논리적 파티션과 오프셋 포함
     - 원본 시스템에서 필요로 하는 파티션과 오프겍
     - 분할하고 오프셋을 추적하는 좋은 방법을 결정
   - 카프카의 브로커에 워커가 레코드를 보냄 - 쓴 뒤 오프셋 저장
   - 싱크 커넥터 : 토픽, 파티션, 오프셋 식별자가 이미 포함되어 있는 카프카 레코드를 읽은뒤 put() 메서드를 이용해 주어졌던 오프섹을 카프카에 커밋

## 9.4 카프카 커넥트의 대안

1. 다른 데이터 저장소를 위한 수집 프레임워크
   - 자체적인 데이터 수집 툴 : 하둡(플룸), 엘라스틱서치(logstash, fluentd)
2. GUI 기반 ETL 툴
   - 탈렌드 Talend, 펜타호 Pentaho, 아파치 나이파이 Nifi, 스트림세츠 Streamsets
   - 대개 복잡한 워크플로를 상정하고 무겁고 복잡
3. 스트림 프로세싱 프레임워크
   - 처리된 이벤트를 바로 쓰기 (중간에 카프카에 저장할 필요가 없이)
   - 메시지 유실이나 오염의 단점

## 9.5 요약

- 데이터 통합관점에서 카프카를 사용해야 하는 이유
- 카프카 커넥트
- 장애 발생시 어떻게 모든 데이터를 전달할 수 있을 것인지
